# nginx proxy address - http://172.17.0.1:59042

name: ollama
x-logging: &default-logging
  options:
    max-size: '5m'
    max-file: '1'
  driver: json-file

services:
  ui:
    image: 'ghcr.io/open-webui/open-webui:main'
    container_name: ollama-ui
    hostname: ollama-ui
    restart: always
    pull_policy: always
    logging: *default-logging
    environment:
      - TZ=${TZ:-America/New_York}
      - ANONYMIZED_TELEMETRY=true
      - OLLAMA_BASE_URL=http://ollama-app:11434
    ports:
      - 172.17.0.1:59042:8080
    volumes:
      - './rootfs/data/ui:/app/backend/data'
    depends_on:
      - app
      - comfy
      - diffusion
    networks:
      - ollama

  app:
    image: 'ollama/ollama:latest'
    container_name: ollama-app
    hostname: ollama-app
    restart: always
    pull_policy: always
    tty: true
    logging: *default-logging
    environment:
      - TZ=${TZ:-America/New_York}
      - OLLAMA_MODELS=/data/models
    expose:
      - 11434
    volumes:
      - './rootfs/data/models:/data/models'
      - './rootfs/data/ollama:/root/.ollama'
    devices:
      - /dev/dri
    networks:
      - ollama

  comfy:
    image: 'ghcr.io/ai-dock/comfyui:v2-cpu-22.04-v0.2.7'
    container_name: ollama-comfy
    hostname: ollama-comfy
    restart: always
    pull_policy: always
    logging: *default-logging
    environment:
      - TZ=${TZ:-America/New_York}
      - COMFYUI_ARGS=--cpu
      - WEB_ENABLE_AUTH=false
      - CF_QUICK_TUNNELS=false
      - USE_PERSISTENT_DATA=true
      - DIRECT_ADDRESS_GET_WAN=true
    expose:
      - 8188
    volumes:
      - './rootfs/data/comfy:/workspace'
      - './rootfs/data/models:/app/models'
    devices:
      - /dev/dri
    networks:
      - ollama

networks:
  ollama:
    name: ollama
    external: false
